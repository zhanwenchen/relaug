# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
from torch import no_grad as torch_no_grad, stack as torch_stack
from torch.distributed import reduce, get_rank
from maskrcnn_benchmark.utils.comm import get_world_size


@torch_no_grad()
def reduce_loss_dict(loss_dict):
    """
    Reduce the loss dictionary from all processes so that process with rank
    0 has the averaged results. Returns a dict with the same fields as
    loss_dict, after reduction.
    """
    world_size = get_world_size()
    if world_size < 2:
        return loss_dict
    loss_names = []
    all_losses = []
    for k in sorted(loss_dict.keys()):
        loss_names.append(k)
        all_losses.append(loss_dict[k])
    all_losses = torch_stack(all_losses, dim=0)
    reduce(all_losses, dst=0)
    if get_rank() == 0:
        # only main process gets accumulated, so only divide by
        # world_size in this case
        all_losses /= world_size
    reduced_losses = dict(zip(loss_names, all_losses))
    return reduced_losses
